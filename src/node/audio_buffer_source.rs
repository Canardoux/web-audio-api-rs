use std::sync::atomic::{Ordering, AtomicBool};
use std::sync::Arc;
use crossbeam_channel::{Receiver, Sender};

use crate::audio_buffer::{AudioBuffer};
use crate::context::{AsBaseAudioContext, AudioContextRegistration, AudioParamId};
use crate::param::{AudioParam, AudioParamOptions, AutomationRate};
use crate::render::{AudioParamValues, AudioProcessor, AudioRenderQuantum};
use crate::{SampleRate, RENDER_QUANTUM_SIZE, AtomicF64};

use super::{
    AudioNode, ChannelConfig, ChannelConfigOptions,
};

struct BufferChannelsMessage(Vec<Arc<Vec<f32>>>, f64);

pub struct AudioBufferSourceOptions {
    pub buffer: Option<AudioBuffer>,
    // pub detune: f32,
    // pub r#loop: bool,
    // pub loop_start: f64,
    // pub loop_end: f64,
    // pub playback_rate: f32,
    pub channel_config: ChannelConfigOptions,
}

impl Default for AudioBufferSourceOptions {
    fn default() -> Self {
        Self {
            buffer: None,
            // detune: 0.,
            // r#loop: false,
            // loop_start: 0.,
            // loop_end: 0.,
            // playback_rate: 1.,
            channel_config: Default::default(),
        }
    }
}

// store attributes that must be shared between the node and the renderer
// @note - kind of pack both Scheduler and Controller with an additionnal
// `duration` entry and `seek` renamed to `offset`
//
// @note - this could probably be generated by macro! (`Vec<{name, default<type>}>`)
//         another macro could also generate Getters and Setters on the node itself
#[derive(Clone, Debug)]
struct SharedAttributes {
    start: Arc<AtomicF64>,
    stop: Arc<AtomicF64>,
    offset: Arc<AtomicF64>,
    duration: Arc<AtomicF64>,
    loop_: Arc<AtomicBool>,
    loop_start: Arc<AtomicF64>,
    loop_end: Arc<AtomicF64>,
}

impl Default for SharedAttributes {
    fn default() -> Self {
        Self {
            start: Arc::new(AtomicF64::new(0.)),
            stop: Arc::new(AtomicF64::new(f64::MAX)),
            offset: Arc::new(AtomicF64::new(0.)),
            duration: Arc::new(AtomicF64::new(f64::MAX)),
            loop_: Arc::new(AtomicBool::new(false)),
            loop_start: Arc::new(AtomicF64::new(0.)),
            loop_end: Arc::new(AtomicF64::new(0.)),
        }
    }
}

impl SharedAttributes {
    fn get_start(&self, ) -> f64 {
        self.start.load()
    }
    fn set_start(&self, value: f64) {
        self.start.store(value);
    }

    fn get_stop(&self, ) -> f64 {
        self.stop.load()
    }

    fn set_stop(&self, value: f64) {
        self.stop.store(value);
    }

    fn get_offset(&self, ) -> f64 {
        self.offset.load()
    }

    fn set_offset(&self, value: f64) {
        self.offset.store(value);
    }

    fn get_duration(&self, ) -> f64 {
        self.duration.load()
    }

    fn set_duration(&self, value: f64) {
        self.duration.store(value);
    }

    fn get_loop(&self, ) -> bool {
        self.loop_.load(Ordering::SeqCst)
    }

    // fn set_loop(&self, value: bool) {
    //     self.loop_.store(value, Ordering::SeqCst);
    // }

    fn get_loop_start(&self, ) -> f64 {
        self.loop_start.load()
    }

    // fn set_loop_start(&self, value: f64) {
    //     self.loop_start.store(value);
    // }

    fn get_loop_end(&self, ) -> f64 {
        self.loop_end.load()
    }

    // fn set_loop_end(&self, value: f64) {
    //     self.loop_end.store(value);
    // }
}

pub struct AudioBufferSourceNode {
    registration: AudioContextRegistration,
    attributes: SharedAttributes,
    channel_config: ChannelConfig,
    sender: Sender<BufferChannelsMessage>,
    detune: AudioParam, // has constraints, no a-rate
    playback_rate: AudioParam, // has constraints, no a-rate
    buffer: Option<AudioBuffer>,
    start_called: bool,
}

impl AudioNode for AudioBufferSourceNode {
    fn registration(&self) -> &AudioContextRegistration {
        &self.registration
    }

    fn channel_config_raw(&self) -> &ChannelConfig {
        &self.channel_config
    }

    fn number_of_inputs(&self) -> u32 {
        0
    }
    fn number_of_outputs(&self) -> u32 {
        1
    }
}

impl AudioBufferSourceNode {
    pub fn new<C: AsBaseAudioContext>(context: &C, options: AudioBufferSourceOptions) -> Self {
        // create render and register all that in the context
        context.base().register(move |registration| {
            // @todo - hndle options
            // let AudioBufferSourceOptions {
            //   buffer,
            //   channel_config,
            // } = options;

            // create parameters - both are parameters k-rate and can't be changed
            // @see - <https://webaudio.github.io/web-audio-api/#audioparam-automation-rate-constraints>
            // @todo - see what it means for us
            let detune_param_options = AudioParamOptions {
                min_value: f32::MIN,
                max_value: f32::MAX,
                default_value: 0.,
                automation_rate: AutomationRate::K,
            };
            let (d_param, d_proc) = context
                .base()
                .create_audio_param(detune_param_options, registration.id());

            let playback_rate_param_options = AudioParamOptions {
                min_value: f32::MIN,
                max_value: f32::MAX,
                default_value: 1.,
                automation_rate: AutomationRate::K,
            };
            let (pr_param, pr_proc) = context
                .base()
                .create_audio_param(playback_rate_param_options, registration.id());

            // for sending buffer channels references to the renderer
            //
            // @note: Maybe we don't want to block the control thread waiting for next
            // `render.tick`? However it's probably better to block the control thread
            // rather than the render thread...
            // Wouldn't be a possible solution to spawn a middle man thread that could block?
            // ```
            // let (sender, receiver) = crossbeam_channel::bounded(0);
            // thread::spawn(move || {
            //     sender.send(data); // this thread would block waiting for the render thread
            // });
            // ```
            // ...but maybe this could introduce strange race conditions, I don't know
            //
            // decision 1 - so let's go for blocking control for now...
            // decision 2 - for some reason it blocks the rendering in tests w/ offline audio context
            let (sender, receiver) = crossbeam_channel::bounded(1);

            // use lower level stuff, `scheduler` and `controller` is not suited
            // as we also need to store a `duration` here and `seek` should be
            // renamed to `offset`, also leads to a cleaner API IMO.
            // @note - maybe the approach could be generalized w/ a macro
            let shared_attributes = SharedAttributes::default();

            let renderer = AudioBufferSourceRenderer {
                attributes: shared_attributes.clone(),
                receiver,
                buffer: None,
                buffer_duration: None,
                detune: d_proc,
                playback_rate: pr_proc,
                render_state: AudioBufferRendererState::default(),
            };

            // create node
            let mut node = Self {
                registration,
                attributes: shared_attributes,
                channel_config: options.channel_config.into(),
                sender,
                detune: d_param,
                playback_rate: pr_param,
                buffer: None, // @note - go through `set_buffer` even if `options.buffer`
                start_called: false,
            };

            if options.buffer.is_some() {
                node.set_buffer(&options.buffer.unwrap());
            }

            (node, Box::new(renderer))
        })
    }

    pub fn set_buffer(&mut self, audio_buffer: &AudioBuffer) {
        // - Let new buffer be the AudioBuffer or null value to be assigned to buffer.
        // - If new buffer is not null and [[buffer set]] is true, throw an InvalidStateError and abort these steps.
        if self.buffer.is_some() {
            // buffer has already been set, panic!
            panic!("InvalidStateError - cannot assign buffer twice");
        }
        // - If new buffer is not null, set [[buffer set]] to true.
        // - Assign new buffer to the buffer attribute.
        self.buffer = Some(audio_buffer.clone());
        // - If start() has previously been called on this node, perform the operation acquire the content on buffer.
        if self.start_called {
            self.acquire_buffer_channels();
        }
    }

    // scheduling methods
    pub fn start(&mut self) {
        let start = self.registration.context().current_time();
        self.start_at(start);
    }

    pub fn start_at(&mut self, start: f64) {
        if self.start_called {
            panic!("Cannot `start` twice");
        }

        self.start_called = true;
        self.acquire_buffer_channels();
        self.attributes.set_start(start);
    }

    pub fn start_at_with_offset(&mut self, start: f64, offset: f64) {
        self.attributes.set_offset(offset);
        self.start_at(start);
    }

    pub fn start_at_with_offset_and_duration(&mut self, start: f64, offset: f64, duration: f64) {
        self.attributes.set_offset(offset);
        self.attributes.set_duration(duration);
        self.start_at(start);
    }

    pub fn stop(&mut self) {
        let stop = self.registration.context().current_time();
        self.stop_at(stop);
    }

    pub fn stop_at(&mut self, stop: f64) {
        // https://webaudio.github.io/web-audio-api/#dom-audioscheduledsourcenode-stop
        if self.start_called == false {
            panic!("InvalidStateError cannot stop before start");
        }

        self.attributes.set_stop(stop)
    }

    #[must_use]
    pub const fn playback_rate(&self) -> &AudioParam {
        &self.playback_rate
    }

    #[must_use]
    pub const fn detune(&self) -> &AudioParam {
        &self.detune
    }

    // @todo - attributes accessors (loop, loopStart, loopEnd)

    // create a new Vec<Arc<Vec<f32>>> containing cloned Arc references from
    // `AudioBuffer.internal_data`
    // cf. https://webaudio.github.io/web-audio-api/#acquire-the-content
    //
    // @TBC - maybe this step is not necessary as the `Arc`s are already
    // cloned in `set_buffer`?
    fn acquire_buffer_channels(&self) {
        let buffer = self.buffer.as_ref().unwrap();
        let number_of_channels = buffer.number_of_channels();
        let duration = buffer.duration();
        let mut channels = Vec::<Arc<Vec<f32>>>::with_capacity(number_of_channels);

         for channel_number in 0..number_of_channels {
            let channel = buffer.get_channel_clone(channel_number);
            channels.push(channel);
        }

        // @note - maybe we should create a new thread there so that
        self.sender
            .send(BufferChannelsMessage(channels, duration))
            .expect("Sending BufferChannelsMessage failed");
    }
}

struct AudioBufferRendererState {
    buffer_time: f64,
    started: bool,
    entered_loop: bool,
    buffer_time_elapsed: f64,
}

impl Default for AudioBufferRendererState {
    fn default() -> Self {
        Self {
            buffer_time: 0.,
            started: false,
            entered_loop: false,
            buffer_time_elapsed: 0.,
        }
    }
}

struct AudioBufferSourceRenderer {
    attributes: SharedAttributes,
    receiver: Receiver<BufferChannelsMessage>,
    buffer: Option<Vec<Arc<Vec<f32>>>>,
    buffer_duration: Option<f64>,
    detune: AudioParamId,
    playback_rate: AudioParamId,
    render_state: AudioBufferRendererState,
}

impl AudioProcessor for AudioBufferSourceRenderer {
    fn process(
        &mut self,
        _inputs: &[AudioRenderQuantum], // no input
        outputs: &mut [AudioRenderQuantum],
        params: AudioParamValues,
        timestamp: f64,
        sample_rate: SampleRate,
    ) -> bool {
        // cf. https://webaudio.github.io/web-audio-api/#playback-AudioBufferSourceNode
        // single output node
        let output = &mut outputs[0];

        let dt = 1. / sample_rate.0 as f64;
        let num_frames = RENDER_QUANTUM_SIZE;
        let next_block_time = timestamp + dt * RENDER_QUANTUM_SIZE as f64;

        // check if we received some buffer_channels
        if let Ok(msg) = self.receiver.try_recv() {
            self.buffer = Some(msg.0);
            self.buffer_duration = Some(msg.1);
        }

        let detune_values = params.get(&self.detune);
        let playback_rate_values = params.get(&self.playback_rate);
        // compute compound parameter at k-rate
        let detune = detune_values[0];
        let playback_rate = playback_rate_values[0];
        let computed_playback_rate = (playback_rate * 2_f32.powf(detune / 1200.)) as f64;

        let start_time = self.attributes.get_start();
        let stop_time = self.attributes.get_stop();
        let mut offset = self.attributes.get_offset();
        let duration = self.attributes.get_duration();
        let loop_ = self.attributes.get_loop();
        let loop_start = self.attributes.get_loop_start();
        let loop_end = self.attributes.get_loop_end();

        // will only be used if `loop_` is true anyway, so no need for `Option`
        let mut actual_loop_start = 0.;
        let mut actual_loop_end = 0.;

        // return early if start_time is beyond this block
        if start_time >= next_block_time {
            output.make_silent();
            return true;
        }

        // @todo - not sure this should behave like that
        // probably we should wait for the buffer, let try in a browser
        if self.buffer.is_none() {
            output.make_silent();
            return false;
        }

        // In addition, if the buffer has more than one channel, then the
        // AudioBufferSourceNode output must change to a single channel of silence
        // at the beginning of a render quantum after the time at which any one of
        // the following conditions holds:
        let buffer_duration = self.buffer_duration.unwrap();

        // - the stop time has been reached.
        // - the duration has been reached.
        if timestamp >= stop_time
            || self.render_state.buffer_time_elapsed >= duration
        {
            // println!("reached stopTime or duration, return false");
            output.make_silent();
            return false;
        }

        // - the end of the buffer has been reached.
        if loop_ == false {
            // forward playback rate
            if computed_playback_rate > 0.
                && self.render_state.buffer_time >= buffer_duration
            {
                // println!("reached end of file, return false");
                output.make_silent();
                return false;
            }

            // backward playback rate
            if computed_playback_rate < 0.
                && self.render_state.buffer_time < 0.
            {
                // println!("reached end of file (backward), return false");
                output.make_silent();
                return false;
            }

        }

        let buffer_channels = self.buffer.as_ref().unwrap();
        let num_channels = buffer_channels.len();

        output.set_number_of_channels(num_channels);
        // placeholder to compute samples for each channel at a given index
        // @todo - ok tha't allocation... could be done only once when audio buffer is kown
        let mut buf = Vec::<f32>::with_capacity(num_channels);
        buf.resize(num_channels, 0.); // init with zeros

        // if start_time is aligned on a sample and computed_playback_rate is 1
        // we can just copy samples from the buffer without interpolation.
        // @note - the chosen threshold is arbitrary
        // for info: 1 / 44100 = 0.000_022_67573
        // let is_aligned = (start_time % dt) < 0.000_001;

        // go through the algorithm described in the spec
        // see <https://webaudio.github.io/web-audio-api/#playback-AudioBufferSourceNode>
        let mut current_time = timestamp;

        if loop_ == true && self.buffer.is_some() {
            if loop_start >= 0. && loop_end > 0. && loop_start < loop_end {
                actual_loop_start = loop_start;
                actual_loop_end = loop_end.min(buffer_duration);
            } else {
                actual_loop_start = 0.;
                actual_loop_end = buffer_duration;
            }
        } else {
            self.render_state.entered_loop = false;
        }

        for index in 0..num_frames {
            if current_time < start_time
                || current_time >= stop_time // we have stop tim in current block
                || self.render_state.buffer_time_elapsed >= duration {
                buf.fill(0.);
                output.set_channels_values_at(index, &buf);
                continue; // nothing more to do for this sample
            }

            // we have reached start time
            if self.render_state.started == false {
                if loop_ && computed_playback_rate >= 0. && offset >= actual_loop_end {
                    offset = actual_loop_end;
                }

                if loop_ && computed_playback_rate < 0. && offset < actual_loop_start {
                    offset = actual_loop_start;
                }

                self.render_state.buffer_time = offset;
                self.render_state.started = true;
            }

            if loop_ == true {
                if self.render_state.entered_loop == false {
                    // playback began before or within loop, and playhead is now past loop start
                    if offset < actual_loop_end && self.render_state.buffer_time >= actual_loop_start {
                        self.render_state.entered_loop = true;
                    }

                    // playback began after loop, and playhead is now prior to the loop end
                    // @note - only possible when playback_rate < 0 ?
                    if offset >= actual_loop_end && self.render_state.buffer_time < actual_loop_end {
                        self.render_state.entered_loop = true;
                    }
                }

                // check loop boundaries
                if self.render_state.entered_loop {
                    while self.render_state.buffer_time >= actual_loop_end {
                        self.render_state.buffer_time -= actual_loop_end - actual_loop_start;
                    }

                    while self.render_state.buffer_time < actual_loop_start {
                        self.render_state.buffer_time += actual_loop_end - actual_loop_start;
                    }
                }
            }

            if self.render_state.buffer_time >= 0.
                && self.render_state.buffer_time < buffer_duration {
                // simplest form, get closer sample
                // if start_time aligned on samples && playback_rate = 1
                self.compute_playback_at_position_direct(
                    self.render_state.buffer_time,
                    sample_rate.0 as f64,
                    buffer_channels,
                    &mut buf
                );

                output.set_channels_values_at(index, &buf);
            } else {
                buf.fill(0.);
                output.set_channels_values_at(index, &buf);
            }

            self.render_state.buffer_time += dt * computed_playback_rate;
            self.render_state.buffer_time_elapsed += dt * computed_playback_rate;
            current_time += dt;
        }


        true
    }
}

impl AudioBufferSourceRenderer {
    // just pick the index closer to the given position
    fn compute_playback_at_position_direct(
        &self,
        position: f64,
        sample_rate: f64,
        buffer_channels: &Vec<Arc<Vec<f32>>>,
        buf: &mut [f32]
    ) {
        let sample_index = (position * sample_rate).round() as usize;

        for (channel_index, channel) in buffer_channels.iter().enumerate() {
            buf[channel_index] = channel[sample_index];
        }
    }

    #[allow(dead_code)]
    fn compute_playback_at_position(
        &self,
        _position: f64,
        _sample_rate: f64,
        _buffer_channels: &Vec<Arc<Vec<f32>>>,
        _buf: &mut [f32]
    ) {
        // @todo - linear interpolation
        // cf. https://github.com/WebAudio/web-audio-api/issues/2032#issuecomment-691178080
    }
}

#[cfg(test)]
mod tests {

    use crate::context::{OfflineAudioContext, AsBaseAudioContext};
    use crate::audio_buffer::decode_audio_data;
    use crate::{SampleRate, RENDER_QUANTUM_SIZE};
    use crate::node::{AudioNode};

    use float_eq::assert_float_eq;

    #[test]
    fn type_playing_some_file() {
        let mut context = OfflineAudioContext::new(2, RENDER_QUANTUM_SIZE * 1, SampleRate(44_100));
        // load and decode buffer

        let file = std::fs::File::open("sample.wav").unwrap();
        // the Rc should be abstracted somehow, but can't find any solution for now
        let audio_buffer = decode_audio_data(file);

        let mut src = context.create_buffer_source();
        src.set_buffer(&audio_buffer);
        src.connect(&context.destination());
        src.start_at(context.current_time());
        src.stop_at(context.current_time() + 128.);

        let res = context.start_rendering();

        // println!("buffer duration: {:?}", audio_buffer.duration());
        // println!("context sample rate: {:?}", context.sample_rate());

        // right
        assert_float_eq!(
            res.channel_data(0).as_slice(),
            audio_buffer.get_slice(0, 0, 128),
            abs_all <= 0.
        );

        // left
        assert_float_eq!(
            res.channel_data(1).as_slice(),
            audio_buffer.get_slice(1, 0, 128),
            abs_all <= 0.
        );
    }
}
















